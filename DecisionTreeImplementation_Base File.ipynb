{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Implemented Decision Tree Classifier for iris dataset of sklearn\n",
    "### Made BY:- Arpnik Singh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considering all the columns to have discrete value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame formation\n",
    "df = pd.DataFrame(iris.data)\n",
    "df.columns = [\"sl\", \"sw\", 'pl', 'pw']\n",
    "y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find label for a value\n",
    "#if MIN_Value <=val < (m + Mean_Value) / 2 then it is assigned label a\n",
    "#if (m + Mean_Value) <=val < Mean_Value then it is assigned label b\n",
    "#if (Mean_Value) <=val < (Mean_Value + MAX_Value)/2 then it is assigned label c\n",
    "#if (Mean_Value + MAX_Value)/2 <=val <= MAX_Value  then it is assigned label d\n",
    "\n",
    "def label(val, *boundaries):\n",
    "    if (val < boundaries[0]):\n",
    "        return 'a'\n",
    "    elif (val < boundaries[1]):\n",
    "        return 'b'\n",
    "    elif (val < boundaries[2]):\n",
    "        return 'c'\n",
    "    else:\n",
    "        return 'd'\n",
    "\n",
    "#Function to convert a continuous data into labelled data\n",
    "#There are 4 lables  - a, b, c, d\n",
    "def toLabel(df, old_feature_name):\n",
    "    second = df[old_feature_name].mean()\n",
    "    minimum = df[old_feature_name].min()\n",
    "    first = (minimum + second)/2\n",
    "    maximum = df[old_feature_name].max()\n",
    "    third = (maximum + second)/2\n",
    "    return df[old_feature_name].apply(label, args= (first, second, third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all columns to labelled data\n",
    "df['sl_labeled'] = toLabel(df, 'sl')\n",
    "df['sw_labeled'] = toLabel(df, 'sw')\n",
    "df['pl_labeled'] = toLabel(df, 'pl')\n",
    "df['pw_labeled'] = toLabel(df, 'pw')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['sl', 'sw', 'pl', 'pw'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['sl_labeled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127, 4), (23, 4), (127,), (23,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spliting dataframe into train and test sets\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(df,y,test_size=0.15)\n",
    "xtrain.shape,xtest.shape,ytrain.shape,ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of node of Decision Tree\n",
    "#col_name=column name to split upon\n",
    "#isleaf is boolean value for detecting leaf node\n",
    "#child nodes are stored in form of dictionary where key is the distinct value of column\n",
    "#and value is pointer to a Tree Node \n",
    "#count stores total count of data points each node irrespective of their target class\n",
    "class TreeNode:\n",
    "    def __init__(self,column):\n",
    "        self.col_name=column\n",
    "        self.children=dict()\n",
    "        self.isleaf=False\n",
    "        self.count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns entropy of a node given the target labels of data points given as numpy array\n",
    "def entropy(y):\n",
    "    grp,cnt=np.unique(y,return_counts=True)\n",
    "    total=sum(cnt)\n",
    "    ent=0.000\n",
    "    for i in range(grp.shape[0]):\n",
    "        ent-=((cnt[i]/total)*np.log(cnt[i]/total))\n",
    "    return ent;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns split information given a list containing number of data points in each child node after split\n",
    "def split_info(t):\n",
    "    tot=sum(t)\n",
    "    info=0.0\n",
    "    for i in t:\n",
    "        info-=((i/tot)*np.log(i/tot))\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return gain ratio and entropy after split for a particular column\n",
    "#col is name of the column\n",
    "def current_column(x,y,col):\n",
    "    grp,cnt=np.unique(x.loc[:,col],return_counts=True)\n",
    "    #split information\n",
    "    denom=split_info(cnt)\n",
    "    #current entropy\n",
    "    entc=entropy(y)\n",
    "    split_ent=0.0\n",
    "    tot=sum(cnt)\n",
    "    for i in range(grp.shape[0]):\n",
    "        temp=entropy(y[x.loc[:,col]==grp[i]])\n",
    "        #weighted average of each entropy of child\n",
    "        split_ent+=((cnt[i]*temp)/tot)\n",
    "    #information gain\n",
    "    infog=entc-split_ent\n",
    "    #gain ratio\n",
    "    gratio=infog/denom\n",
    "    return gratio,split_ent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building decision tree in depth first manner [hence the value of level]\n",
    "#unsused is a list containing names of features left to split upon\n",
    "def decision_tree(x,y,unused,count):\n",
    "    #number of features left\n",
    "    features=len(unused)\n",
    "    print(\"Level\",count)\n",
    "    count+=1;\n",
    "    #base case when pure class or no feature \n",
    "    if(np.unique(y).shape[0]<=1 or len(unused)==0):\n",
    "        cls,num=np.unique(y,return_counts=True)\n",
    "        for i in range(cls.shape[0]):\n",
    "            print(\"Count of class\",cls[i],'is',num[i])\n",
    "        print(\"Current Entropy is\",entropy(y))\n",
    "        print(\"Reached a leaf Node\")\n",
    "        #print('unused=',len(unused))\n",
    "        print()\n",
    "        print()\n",
    "        root=TreeNode('Predicted value:'+str(cls[np.argmax(num)]))\n",
    "        root.isleaf=True\n",
    "        root.predict=cls[np.argmax(num)]\n",
    "        root.count=y.shape[0]\n",
    "        return root;\n",
    "    \n",
    "    #bestcol is final decided column split upon\n",
    "    #mgratio is maximum gain ratio\n",
    "    #entc is current node entropy\n",
    "    #mentc is entropy after split corresponding to maximum gain entropy\n",
    "    bestcol=''\n",
    "    mgratio=-999999999.000\n",
    "    entc=entropy(y);\n",
    "    mentc=-99999999.00\n",
    "    #finding optimal feature/column to split upon\n",
    "    for i in unused:\n",
    "        cur_gratio,cur_split_entc=current_column(x,y,i)\n",
    "        #print(i,cur_gratio)\n",
    "        if(mgratio<cur_gratio):\n",
    "            mgratio=cur_gratio\n",
    "            bestcol=i;\n",
    "            mentc=cur_split_entc\n",
    "    print('Current Entropy:',entc)\n",
    "    print('Feature used to split upon:',bestcol)\n",
    "    print('Gain ratio after split:',mgratio)\n",
    "    print('Net entropy after split is:',mentc)\n",
    "    print()\n",
    "    print()\n",
    "    #removing split column\n",
    "    unused.remove(bestcol)\n",
    "    #make node\n",
    "    root=TreeNode(bestcol)\n",
    "    root.count=y.shape[0]\n",
    "    cls,cnt=np.unique(y,return_counts=True)\n",
    "    root.predict=cls[np.argmax(cnt)]\n",
    "    for i in np.unique(x.loc[:,bestcol]):\n",
    "        #creating data frame of child nodes\n",
    "        tempx=x[x.loc[:,bestcol]==i].copy()\n",
    "        tempy=y[x.loc[:,bestcol]==i].copy()\n",
    "        del tempx[bestcol]\n",
    "        #recursively creating tree\n",
    "        temp_node=decision_tree(tempx,tempy,unused,count);\n",
    "        root.children[i]=temp_node\n",
    "    return root;\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Current Entropy: 1.0981798219296024\n",
      "Feature used to split upon: pl_labeled\n",
      "Gain ratio after split: 0.6896548049778171\n",
      "Net entropy after split is: 0.26485471572975466\n",
      "\n",
      "\n",
      "Level 1\n",
      "Count of class 0 is 41\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 1\n",
      "Count of class 1 is 7\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 1\n",
      "Current Entropy: 0.6228990536607193\n",
      "Feature used to split upon: pw_labeled\n",
      "Gain ratio after split: 0.4160394085353637\n",
      "Net entropy after split is: 0.3100666269498895\n",
      "\n",
      "\n",
      "Level 2\n",
      "Count of class 1 is 4\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 2\n",
      "Current Entropy: 0.4293230219306162\n",
      "Feature used to split upon: sl_labeled\n",
      "Gain ratio after split: 0.12603900774175458\n",
      "Net entropy after split is: 0.31491696825686727\n",
      "\n",
      "\n",
      "Level 3\n",
      "Count of class 2 is 1\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 3\n",
      "Count of class 1 is 12\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 3\n",
      "Current Entropy: 0.5117400734174093\n",
      "Feature used to split upon: sw_labeled\n",
      "Gain ratio after split: 0.10292922040545853\n",
      "Net entropy after split is: 0.429106562055382\n",
      "\n",
      "\n",
      "Level 4\n",
      "Count of class 1 is 3\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 4\n",
      "Count of class 1 is 12\n",
      "Count of class 2 is 5\n",
      "Current Entropy is 0.605797499372304\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 4\n",
      "Count of class 1 is 4\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 3\n",
      "Count of class 1 is 2\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 2\n",
      "Count of class 2 is 11\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 1\n",
      "Count of class 2 is 25\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#building the decision tree and returning the head node\n",
    "root=decision_tree(xtrain,ytrain,list(xtrain.columns),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Decision-Tree.gv.pdf'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing decision Tree\n",
    "tree=Digraph('Decision-Tree')\n",
    "if(root.isleaf):\n",
    "    tree.attr('node',shape='circle')\n",
    "else:\n",
    "    tree.attr('node',shape='square')\n",
    "\n",
    "temp=root.col_name\n",
    "tree.node(temp)\n",
    "def draw_tree(root):\n",
    "    #global tree;\n",
    "    if(root.isleaf):\n",
    "        return;\n",
    "    for i in root.children:\n",
    "        tree.attr('node',shape='square')\n",
    "        temp=root.children[i].col_name\n",
    "        if(root.children[i].isleaf):\n",
    "            tree.attr('node',shape='circle')\n",
    "            temp='class '+root.children[i].col_name[-1]\n",
    "        tree.node(temp)\n",
    "        tree.edge(root.col_name,temp,label=i)\n",
    "        draw_tree(root.children[i])\n",
    "draw_tree(root)\n",
    "tree.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to predict target value for 1 row/data point\n",
    "#return 0,1,2\n",
    "#0 for setosa\n",
    "#1 for versicolor\n",
    "#2 for virginica\n",
    "def predict(x,root):\n",
    "    #if(root==None):\n",
    "    #    return None\n",
    "    \n",
    "    #base case when child node\n",
    "    if(root.isleaf):\n",
    "        return root.col_name[-1]\n",
    "    \n",
    "    #print(root.col_name,x.loc[root.col_name])\n",
    "    #print(root.children)\n",
    "    \n",
    "    #if the data point is some how unique(which can be due to lack of data or some other reason) and we don't have an edge corresponding to that value\n",
    "    #return value of that node\n",
    "    if x.loc[root.col_name] not in root.children.keys():\n",
    "        return root.predict\n",
    "    \n",
    "    #match the value of column and move further down in the tree\n",
    "    return predict(x,root.children[x.loc[root.col_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to predict target class for testing data\n",
    "#returns a numpy array\n",
    "def predictions(x,root):\n",
    "    y=np.array([])\n",
    "    for i in range(x.shape[0]):\n",
    "        #print(i,x.iloc[i,:])\n",
    "        y=np.append(y,[int(predict(x.iloc[i,:],root))])\n",
    "        #print(i,y)\n",
    "        #break\n",
    "    return y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_predict=predictions(xtest,root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "2 2\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "2 2\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "2 2\n",
      "0 0\n",
      "2 1\n",
      "2 2\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "2 1\n",
      "1 1\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "#manual checking of values\n",
    "for i in range(ytest.shape[0]):\n",
    "    print(ytest[i],int(ytest_predict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9 0 0]\n",
      " [0 6 0]\n",
      " [0 2 6]]\n",
      "\n",
      "\n",
      "F1 Score of each class (in this order 'setosa', 'versicolor', 'virginica')\n",
      "[1.         0.85714286 0.85714286]\n",
      "\n",
      "Precision for each class (in this order 'setosa', 'versicolor', 'virginica')\n",
      "[1.   0.75 1.  ]\n",
      "\n",
      "Recall of each class (in this order 'setosa', 'versicolor', 'virginica')\n",
      "[1.   1.   0.75]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(ytest,ytest_predict))\n",
    "print()\n",
    "print()\n",
    "print(\"F1 Score of each class (in this order 'setosa', 'versicolor', 'virginica')\")\n",
    "print(f1_score(ytest,ytest_predict,average=None))\n",
    "print()\n",
    "print(\"Precision for each class (in this order 'setosa', 'versicolor', 'virginica')\")\n",
    "print(precision_score(ytest,ytest_predict,average=None))\n",
    "print()\n",
    "print(\"Recall of each class (in this order 'setosa', 'versicolor', 'virginica')\")\n",
    "print(recall_score(ytest,ytest_predict,average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considering all the columns to have continous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': '/home/yay/anaconda3/lib/python3.7/site-packages/sklearn/datasets/data/iris.csv'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=datasets.load_iris()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "X=data.data\n",
    "Y=data.target\n",
    "col=data.feature_names\n",
    "target_class=data.target_names\n",
    "print(X.shape,Y.shape)\n",
    "print(data.DESCR)\n",
    "X=pd.DataFrame(X,columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((135, 4), (15, 4), (135,), (15,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spliting data to train and test set\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,Y,test_size=0.1)\n",
    "xtrain.shape,xtest.shape,ytrain.shape,ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree is build assuming only binary cuts are possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node of decision tree\n",
    "#cut is the value at which continous vale mark is made\n",
    "class TreeNode:\n",
    "    def __init__(self,column,mark):\n",
    "        self.col_name=column\n",
    "        self.left=None\n",
    "        self.right=None\n",
    "        self.cut=mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding entropy of a node \n",
    "def entropy(y):\n",
    "    grp,cnt=np.unique(y,return_counts=True)\n",
    "    total=sum(cnt)\n",
    "    ent=0.000\n",
    "    for i in range(grp.shape[0]):\n",
    "        ent-=((cnt[i]/total)*np.log(cnt[i]/total))\n",
    "    return ent;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split info of child nodes of left and right sub tree\n",
    "def split_info(t1,t2):\n",
    "    tot=t1+t2\n",
    "    info=-((t1/tot)*np.log(t1/tot))\n",
    "    info-=((t2/tot)*np.log(t2/tot))\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding optimal cut value for a particular column\n",
    "#returns max gain ratio,cut value and entropy after split (at max gain ratio) for a particular column\n",
    "def column_seperator(x,y,col):\n",
    "    #sorting data frame according to column values\n",
    "    indices=np.argsort(x.loc[:,col])\n",
    "    x=x.iloc[indices,:]\n",
    "    y=y[indices]\n",
    "    #current entropy\n",
    "    entc=entropy(y)\n",
    "    x.reset_index(drop=True,inplace=True)\n",
    "    max_gratio=-999999999\n",
    "    max_ent=-999999\n",
    "    sep=-99999999\n",
    "    #traversing at every mid point of the column\n",
    "    for i in range(1,x.shape[0]):\n",
    "        mark=(x.loc[i,col]+x.loc[i-1,col])/2\n",
    "        indices=(x.loc[:,col]>mark)\n",
    "        ent1=entropy(y[indices])\n",
    "        tot1=y[indices].shape[0]\n",
    "        indices=(x.loc[:,col]<=mark)\n",
    "        ent2=entropy(y[indices])\n",
    "        tot2=y[indices].shape[0]\n",
    "        total=tot1+tot2\n",
    "        #entropy after split\n",
    "        finalent=(tot1*ent1+tot2*ent2)/total\n",
    "        #information gain\n",
    "        infog=entc-finalent\n",
    "        spt_info=split_info(tot1,tot2)\n",
    "        #gain ratio\n",
    "        gratio=infog/spt_info\n",
    "        if(max_gratio<gratio):\n",
    "            max_gratio=gratio\n",
    "            sep=mark\n",
    "            max_ent=finalent\n",
    "    return max_gratio,sep,max_ent\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "#build Decision tree\n",
    "def decision_tree(x,y):\n",
    "    global count\n",
    "    #number of features left\n",
    "    features=x.shape[1]\n",
    "    print(\"Level\",count)\n",
    "    count+=1;\n",
    "    #base case when no feature or pure class\n",
    "    if(np.unique(y).shape[0]<=1 or x.shape[1]==0):\n",
    "        cls,num=np.unique(y,return_counts=True)\n",
    "        for i in range(cls.shape[0]):\n",
    "            print(\"Count of class\",cls[i],'is',num[i])\n",
    "        print(\"Current Entropy is\",entropy(y))\n",
    "        print(\"Reached a leaf Node\")\n",
    "        print()\n",
    "        print()\n",
    "        root=TreeNode('Child Node predicted value:'+str(cls[np.argmax(num)]),0.0)\n",
    "        root.left=root.right=None\n",
    "        return root;\n",
    "    \n",
    "    #splitcol is column decided to split upon\n",
    "    #mgratio is maximum gain ratio\n",
    "    #entc is cureent entropy\n",
    "    #cut is mark at which continous value of column is breaked \n",
    "    splitcol=''\n",
    "    cut=-9999999.000\n",
    "    mgratio=-999999999.000\n",
    "    entc=entropy(y);\n",
    "    mentc=-99999999.00\n",
    "    #finding optimal feature/column to split upon\n",
    "    for i in x.columns:\n",
    "        cur_gratio,mark,cur_entc=column_seperator(x,y,i)\n",
    "        if(mgratio<cur_gratio):\n",
    "            mgratio=cur_gratio\n",
    "            splitcol=i;\n",
    "            cut=mark;\n",
    "            mentc=cur_entc\n",
    "    print('Current Entropy:',entc)\n",
    "    print('Gain ratio:',mgratio)\n",
    "    print('Column to split upon:',splitcol)\n",
    "    print('cut for this column at',cut)\n",
    "    print()\n",
    "    print()\n",
    "    x1=x[x.loc[:,splitcol]<=cut]\n",
    "    x2=x[x.loc[:,splitcol]>cut]\n",
    "    \n",
    "    #Making tree\n",
    "    root=TreeNode(splitcol,cut)\n",
    "    root.left=decision_tree(x1,y[x.loc[:,splitcol]<=cut])\n",
    "    root.right=decision_tree(x2,y[x.loc[:,splitcol]>cut])\n",
    "    return root;\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yay/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "/home/yay/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Entropy: 1.0981146787953189\n",
      "Gain ratio: 0.9999999999999997\n",
      "Column to split upon: petal length (cm)\n",
      "cut for this column at 1.9\n",
      "\n",
      "\n",
      "Level 1\n",
      "Count of class 0 is 46\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 2\n",
      "Current Entropy: 0.6925789628408865\n",
      "Gain ratio: 0.7818745691179838\n",
      "Column to split upon: petal width (cm)\n",
      "cut for this column at 1.7\n",
      "\n",
      "\n",
      "Level 3\n",
      "Current Entropy: 0.2787693717685874\n",
      "Gain ratio: 0.5411340426841159\n",
      "Column to split upon: sepal length (cm)\n",
      "cut for this column at 7.1\n",
      "\n",
      "\n",
      "Level 4\n",
      "Current Entropy: 0.23032354087587759\n",
      "Gain ratio: 0.268351493307326\n",
      "Column to split upon: petal length (cm)\n",
      "cut for this column at 4.9\n",
      "\n",
      "\n",
      "Level 5\n",
      "Current Entropy: 0.10656595882801999\n",
      "Gain ratio: 1.0\n",
      "Column to split upon: petal width (cm)\n",
      "cut for this column at 1.6\n",
      "\n",
      "\n",
      "Level 6\n",
      "Count of class 1 is 44\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 7\n",
      "Count of class 2 is 1\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 8\n",
      "Current Entropy: 0.6931471805599453\n",
      "Gain ratio: 1.0\n",
      "Column to split upon: petal width (cm)\n",
      "cut for this column at 1.5\n",
      "\n",
      "\n",
      "Level 9\n",
      "Count of class 2 is 2\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 10\n",
      "Count of class 1 is 2\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 11\n",
      "Count of class 2 is 1\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n",
      "Level 12\n",
      "Count of class 2 is 39\n",
      "Current Entropy is 0.0\n",
      "Reached a leaf Node\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#start node of decision tree\n",
    "root=decision_tree(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest.reset_index(inplace=True,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to predict target value for 1 row/data point\n",
    "def predict(x,root):\n",
    "    #base case when child node\n",
    "    #print(x.loc[root.col_name],root.cut)\n",
    "    if(root.col_name.startswith('Child')):\n",
    "        return root.col_name[-1]\n",
    "    \n",
    "    if(x.loc[root.col_name]>root.cut):\n",
    "        return predict(x,root.right)\n",
    "    else:\n",
    "        return predict(x,root.left)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to predict targte values for test set\n",
    "def predictions(x,root):\n",
    "    y=np.array([])\n",
    "    for i in range(x.shape[0]):\n",
    "        y=np.append(y,[int(predict(x.iloc[i,:],root))])\n",
    "        #break\n",
    "    return y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_predict=predictions(xtest,root)\n",
    "ytest_predict=ytest_predict.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "2 1\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "0 0\n",
      "2 2\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "#manual checking\n",
    "for i in range(ytest.shape[0]):\n",
    "    print(ytest_predict[i],ytest[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[4 0 0]\n",
      " [0 3 1]\n",
      " [0 0 7]]\n",
      "\n",
      "\n",
      "F1 Score of each class (in this order 'setosa', 'versicolor', 'virginica')\n",
      "[1.         0.85714286 0.93333333]\n",
      "\n",
      "Precision for each class (in this order 'setosa', 'versicolor', 'virginica')\n",
      "[1.    1.    0.875]\n",
      "\n",
      "Recall of each class (in this order 'setosa', 'versicolor', 'virginica')\n",
      "[1.   0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(ytest,ytest_predict))\n",
    "print()\n",
    "print()\n",
    "print(\"F1 Score of each class (in this order 'setosa', 'versicolor', 'virginica')\")\n",
    "print(f1_score(ytest,ytest_predict,average=None))\n",
    "print()\n",
    "print(\"Precision for each class (in this order 'setosa', 'versicolor', 'virginica')\")\n",
    "print(precision_score(ytest,ytest_predict,average=None))\n",
    "print()\n",
    "print(\"Recall of each class (in this order 'setosa', 'versicolor', 'virginica')\")\n",
    "print(recall_score(ytest,ytest_predict,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 0 0]\n",
      " [0 3 1]\n",
      " [0 0 7]]\n"
     ]
    }
   ],
   "source": [
    "#verifying the results with sklearns classifier\n",
    "alg2=DecisionTreeClassifier()\n",
    "alg2.fit(xtrain,ytrain)\n",
    "ypred2=alg2.predict(xtest)\n",
    "print(confusion_matrix(ytest,ypred2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
